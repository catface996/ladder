- ## 1.快速体验K8S
	- 参考 [[K8S官网快速体验K8S]]
- ## 2.安装docker
	- 参考 [[Docker官方安装文档介绍和实战]]
  
## 3. 通过kubeadm安装k8s集群
参考：[[使用Kubeadm安装简单集群实战]]

### 3.2 安装kubeadm
参考：[[使用Kubeadm安装简单集群实战]]

### 3.3 故障排查
参考：[[使用Kubeadm安装简单集群实战]]

### 3.4 使用kubeadm创建集群
参考：[[使用Kubeadm安装简单集群实战]]

### 3.5 进一步验证部署的k8s集群
参考：[[进一步验证部署的k8s集群]]

### 3.6 Kubeadm部署集群的常见故障模拟
#### 仅未关闭swap

~~~shell
## 待执行的命令
kubeadm init --kubernetes-version=1.22.3  \
--apiserver-advertise-address=192.168.162.81   \
--image-repository registry.aliyuncs.com/google_containers  \
--service-cidr=10.10.0.0/16  \
--pod-network-cidr=10.122.0.0/16
## 实际的执行结果
[root@k8s-master-81 ~]# kubeadm init --kubernetes-version=1.22.3  \
> --apiserver-advertise-address=192.168.162.81   \
> --image-repository registry.aliyuncs.com/google_containers  \
> --service-cidr=10.10.0.0/16  \
> --pod-network-cidr=10.122.0.0/16
[init] Using Kubernetes version: v1.22.3
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Swap]: running with swap on is not supported. Please disable swap
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@k8s-master-81 ~]#

~~~
#### 仅未设置vm.swapiness=0


~~~shell
## 待执行的命令
kubeadm init --kubernetes-version=1.22.3  \
--apiserver-advertise-address=192.168.162.81   \
--image-repository registry.aliyuncs.com/google_containers  \
--service-cidr=10.10.0.0/16  \
--pod-network-cidr=10.122.0.0/16
## 实际执行结果
[root@k8s-master-81 ~]# kubeadm init --kubernetes-version=1.22.3  \
> --apiserver-advertise-address=192.168.162.81   \
> --image-repository registry.aliyuncs.com/google_containers  \
> --service-cidr=10.10.0.0/16  \
> --pod-network-cidr=10.122.0.0/16
[init] Using Kubernetes version: v1.22.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master-81 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.162.81]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master-81 localhost] and IPs [192.168.162.81 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master-81 localhost] and IPs [192.168.162.81 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 7.003061 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.22" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master-81 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master-81 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 1oun4f.awoxh841b4wid0st
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.162.81:6443 --token 1oun4f.awoxh841b4wid0st \
	--discovery-token-ca-cert-hash sha256:ce5a9861c06459c88f4833651e0c531862bafdc1522e9664977fafa26070b08c
[root@k8s-master-81 ~]#
~~~
#### 仅未设置SELinux为Permissive

~~~shell
## 待执行的命令
kubeadm init --kubernetes-version=1.22.3  \
--apiserver-advertise-address=192.168.162.81   \
--image-repository registry.aliyuncs.com/google_containers  \
--service-cidr=10.10.0.0/16  \
--pod-network-cidr=10.122.0.0/16
## 实际执行结果
[root@k8s-master-81 ~]# kubeadm init --kubernetes-version=1.22.3  \
> --apiserver-advertise-address=192.168.162.81   \
> --image-repository registry.aliyuncs.com/google_containers  \
> --service-cidr=10.10.0.0/16  \
> --pod-network-cidr=10.122.0.0/16
[init] Using Kubernetes version: v1.22.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master-81 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.162.81]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master-81 localhost] and IPs [192.168.162.81 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master-81 localhost] and IPs [192.168.162.81 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 7.004292 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.22" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master-81 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master-81 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: v224pv.xmg7fph2x9agb86b
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.162.81:6443 --token v224pv.xmg7fph2x9agb86b \
	--discovery-token-ca-cert-hash sha256:f01aa3a80c4321ab51372ebb1489cc5cf117df91a791627f6d9c6aa6be1c4470
[root@k8s-master-81 ~]# getenforce
Enforcing
[root@k8s-master-81 ~]#
~~~

回顾一下安装 kubeadm , kubectl , kubelet

![20220407192627](https://picgo.catface996.com/picgo20220407192627.png)

未关闭SELinux影响的是安装阶段,不是初始化集群阶段

再次执行安装 kubeadm , kubectl , kubelet

~~~shell
sudo yum install -y --nogpgcheck kubelet-1.22.3 kubeadm-1.22.3 kubectl-1.22.3 --disableexcludes=kubernetes
~~~

实际结果,仍然可安装成功
#### 仅未加载br_netfilter

~~~shell
## 临时卸载br_netfilter命令
sudo modprobe -r br_netfilter
## 实际执行结果
[root@k8s-master-81 ~]# lsmod | grep br_netfilter
br_netfilter           22256  0
bridge                151336  1 br_netfilter
[root@k8s-master-81 ~]#
[root@k8s-master-81 ~]# sudo modprobe -r br_netfilter
[root@k8s-master-81 ~]# lsmod | grep br_netfilter
[root@k8s-master-81 ~]#
## 待执行的初始化集群命令
kubeadm init --kubernetes-version=1.22.3  \
--apiserver-advertise-address=192.168.162.81   \
--image-repository registry.aliyuncs.com/google_containers  \
--service-cidr=10.10.0.0/16  \
--pod-network-cidr=10.122.0.0/16
## 实际执行结果
[root@k8s-master-81 ~]# kubeadm init --kubernetes-version=1.22.3  \
> --apiserver-advertise-address=192.168.162.81   \
> --image-repository registry.aliyuncs.com/google_containers  \
> --service-cidr=10.10.0.0/16  \
> --pod-network-cidr=10.122.0.0/16
[init] Using Kubernetes version: v1.22.3
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@k8s-master-81 ~]#
~~~
#### 未配置iptables

~~~shell
## 待执行的命令
kubeadm init --kubernetes-version=1.22.3  \
--apiserver-advertise-address=192.168.162.81   \
--image-repository registry.aliyuncs.com/google_containers  \
--service-cidr=10.10.0.0/16  \
--pod-network-cidr=10.122.0.0/16 

## 实际执行结果
[root@k8s-master-81 ~]# kubeadm init --kubernetes-version=1.22.3  \
> --apiserver-advertise-address=192.168.162.81   \
> --image-repository registry.aliyuncs.com/google_containers  \
> --service-cidr=10.10.0.0/16  \
> --pod-network-cidr=10.122.0.0/16
[init] Using Kubernetes version: v1.22.3
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@k8s-master-81 ~]#
~~~
#### scheduler unhealthy

~~~shell
[root@k8s-master-101 manifests]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                       ERROR
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused
controller-manager   Healthy     ok
etcd-0               Healthy     {"health":"true","reason":""}
~~~

解决方法:
修改 /etc/kubernetes/manifests/ 目录下的 kube-scheduler.yaml 和 kube-controller-manager.yaml,注释掉port=0

~~~shell
[root@k8s-master-101 manifests]# pwd
/etc/kubernetes/manifests
[root@k8s-master-101 manifests]# ll
总用量 16
drwxr-xr-x. 2 root root   79 4月   7 20:20 backup
-rw-------. 1 root root 2272 4月   7 16:59 etcd.yaml
-rw-------. 1 root root 3382 4月   7 16:59 kube-apiserver.yaml
-rw-------. 1 root root 2894 4月   7 20:02 kube-controller-manager.yaml
-rw-------. 1 root root 1480 4月   7 20:03 kube-scheduler.yaml
[root@k8s-master-101 manifests]#
~~~

![20220407202550](https://picgo.catface996.com/picgo20220407202550.png)

然后重启kubelet即可

~~~shell
## 重启kubelet
systemctl restart kubelet

[root@k8s-master-101 manifests]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true","reason":""}
~~~

**切记** 如果要备份文件,备份的文件不能在相同的目录下.
#### docker的cgroup driver和kubelet的cgroup driver不一致

~~~shell
# 执行集群初始化
kubeadm init --kubernetes-version=1.22.3  \
--apiserver-advertise-address=192.168.162.81  \
--image-repository registry.aliyuncs.com/google_containers  \
--service-cidr=10.10.0.0/16  \
--pod-network-cidr=10.122.0.0/16 
~~~

init 日志:
~~~shell
[root@k8s-master-81 ~]# kubeadm init --kubernetes-version=1.22.3  \
> --apiserver-advertise-address=192.168.162.81  \
> --image-repository registry.aliyuncs.com/google_containers  \
> --service-cidr=10.10.0.0/16  \
> --pod-network-cidr=10.122.0.0/16
[init] Using Kubernetes version: v1.22.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master-81 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.162.81]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master-81 localhost] and IPs [192.168.162.81 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master-81 localhost] and IPs [192.168.162.81 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused.

	Unfortunately, an error has occurred:
timed out waiting for the condition

	This error is likely caused by:
- The kubelet is not running
- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

	If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
- 'systemctl status kubelet'
- 'journalctl -xeu kubelet'

	Additionally, a control plane component may have crashed or exited when started by the container runtime.
	To troubleshoot, list all containers using your preferred container runtimes CLI.

	Here is one example how you may list all Kubernetes containers running in docker:
- 'docker ps -a | grep kube | grep -v pause'
Once you have found the failing container, you can inspect its logs with:
- 'docker logs CONTAINERID'

error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
[root@k8s-master-81 ~]#
~~~


kubelet日志:
~~~shell
4月 09 10:27:10 k8s-master-81 kubelet[2350]: I0409 10:27:10.861147    2350 docker_service.go:264] "Docker Info" dockerInfo=&{ID:5NWG:XESZ:TE5Z:ND5P:MKBL:FDHX:CAVB:OSTQ:RLEU:D47J:N4MB:JB25 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem xfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:22 OomKillDisable:true NGoroutines:35 SystemTime:2022-04-09T10:27:10.85210973+08:00 LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion: NEventsListener:0 KernelVersion:3.10.0-1160.el7.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSVersion: OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc000b42310 NCPU:2 MemTotal:3953459200 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:k8s-master-81 Labels:[] ExperimentalBuild:false ServerVersion:19.03.15 ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3df54a852345ae127d1fa3092b95168e4a88e2f8 Expected:3df54a852345ae127d1fa3092b95168e4a88e2f8} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default] ProductLicense: DefaultAddressPools:[] Warnings:[]}
4月 09 10:27:10 k8s-master-81 kubelet[2350]: E0409 10:27:10.861177    2350 server.go:294] "Failed to run kubelet" err="failed to run Kubelet: misconfiguration: kubelet cgroup driver: \"systemd\" is different from docker cgroup driver: \"cgroupfs\""
4月 09 10:27:10 k8s-master-81 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
4月 09 10:27:10 k8s-master-81 systemd[1]: Unit kubelet.service entered failed state.
4月 09 10:27:10 k8s-master-81 systemd[1]: kubelet.service failed.
~~~


### 3.7 kubeadm引导的集群导入rancher

针对三个版本的Kubernetes集群做导入验证,版本分别为:
* 1.18.20
~~~shell
# 安装kubeadm , kubectl , kubelet
sudo yum install -y --nogpgcheck kubelet-1.18.20 kubeadm-1.18.20 kubectl-1.18.20 --disableexcludes=kubernetes
yum list installed | grep kube
# 拉取镜像
kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers  --kubernetes-version=1.18.20
docker images | grep registry.aliyuncs.com
# 引导集群
kubeadm init --kubernetes-version=1.18.20  \
--apiserver-advertise-address=192.168.162.18  \
--image-repository registry.aliyuncs.com/google_containers  \
--service-cidr=10.10.0.0/16  \
--pod-network-cidr=10.122.0.0/16 

kubectl get node
kubectl get pod -A

~~~

* 1.20.15
~~~shell
# 安装kubeadm , kubectl , kubelet
sudo yum install -y --nogpgcheck kubelet-1.20.15 kubeadm-1.20.15 kubectl-1.20.15 --disableexcludes=kubernetes
yum list installed | grep kube
# 拉取镜像
kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers  --kubernetes-version=1.20.15
docker images | grep registry.aliyuncs.com
# 引导集群
kubeadm init --kubernetes-version=1.20.15  \
--apiserver-advertise-address=192.168.162.20  \
--image-repository registry.aliyuncs.com/google_containers  \
--service-cidr=10.10.0.0/16  \
--pod-network-cidr=10.122.0.0/16 

kubectl get node
kubectl get pod -A

~~~


* 1.22.3
~~~shell
# 安装kubeadm , kubectl , kubelet
sudo yum install -y --nogpgcheck kubelet-1.22.3 kubeadm-1.22.3 kubectl-1.22.3 --disableexcludes=kubernetes
yum list installed | grep kube
# 拉取镜像
kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers  --kubernetes-version=1.22.3
docker images | grep registry.aliyuncs.com
# 引导集群
kubeadm init --kubernetes-version=1.22.3  \
--apiserver-advertise-address=192.168.162.22  \
--image-repository registry.aliyuncs.com/google_containers  \
--service-cidr=10.10.0.0/16  \
--pod-network-cidr=10.122.0.0/16 

kubectl get node
kubectl get pod -A

kubeadm join 192.168.162.22:6443 --token xerdgt.jnte2f7rbs0bse5j \
	--discovery-token-ca-cert-hash sha256:4bfa979d801b011ca35984bcfbd8484b1d53106161e8e57290bc048879e0f9a0
~~~

rancher-2.5.12支持的Kubernetes版本如下:
![20220410180539](https://picgo.catface996.com/picgo20220410180539.png)
![20220410180558](https://picgo.catface996.com/picgo20220410180558.png)
![20220410180615](https://picgo.catface996.com/picgo20220410180615.png)
![20220410180634](https://picgo.catface996.com/picgo20220410180634.png)


rancher 中创建三个导入集群
* 导入之前
![20220410182525](https://picgo.catface996.com/picgo20220410182525.png)

* 导入之后
![20220410193700](https://picgo.catface996.com/picgo20220410193700.png)

* 解决 kube-controller-manager和kube-scheduler不健康的问题
* 注释掉 /etc/kubernetes/manifests/ 下的 kube-scheduler.yaml 和 kube-controller-manager.yaml中的 - --port=0
* 重启kubelet


### 3.8 部署和访问Kubernetes仪表盘

Kubernetes官方参考文档地址:

https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/

![image-20220406144249486](https://tva1.sinaimg.cn/large/e6c9d24ely1h0zzxtgnbhj21my0u0jz0.jpg)
#### 部署dashboard



* 可以先下载yaml文件到本地,然后再部署

~~~shell
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml
~~~

* 需要修改 dashboard deployment的nodeSelector
* 对master节点打标签
~~~shell
kubectl label nodes k8s-master-101 node-role=master
~~~

* 修改dashbaord deploymnet的nodeSelector

~~~yaml
nodeSelector:
##"kubernetes.io/os": linux
"node-role": master
~~~

* 部署dashboard

~~~shell
kubectl apply -f recommended.yaml
~~~
#### 暴露dashboard

![image-20220406175600091](https://tva1.sinaimg.cn/large/e6c9d24ely1h105itn4dqj21e10u0tfb.jpg)

~~~shell
## 命令一
kubectl proxy
## 命令二
kubectl proxy --address='192.168.162.51'
## 命令三
kubectl proxy --address='192.168.162.51' --accept-hosts='^*$'
~~~
#### 生成Bearer Token

* 部署ServiceAccount

~~~yaml
apiVersion: v1
kind: ServiceAccount
metadata:
name: admin-user
namespace: kubernetes-dashboard
~~~

* 部署ClusterRoleBinding

~~~yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: admin-user
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: cluster-admin
subjects:
	- kind: ServiceAccount
 name: admin-user
 namespace: kubernetes-dashboard
 ~~~

 * 获取BearerToken

 ~~~shell
 kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"
 ~~~



 踩坑记录

 * 启动代理

 需要注意配置的参数 --address  --accept-hosts='^*$'

 * 访问dashboard提示 io/timeout

 需要修改 dashboard 的Deployment中的nodeSelector

 原有的标签选择器

 ~~~yaml
 nodeSelector:
 ##"kubernetes.io/os": linux
 "node-role": master
 ~~~
## istio 安装部署(参考官方文档)

 官网地址: https://istio.io/

 ![20220410233725](https://picgo.catface996.com/picgo20220410233725.png)

 1. Download and install istio

* 选择哪个版本?  amd? arm? 1.12.4 ? 最新版本?

 阅读下载脚本: https://istio.io/downloadIstio  -->  https://raw.githubusercontent.com/istio/istio/master/release/downloadIstioCandidate.sh

 目前阿里云支持的版本:

 ![20220410233649](https://picgo.catface996.com/picgo20220410233649.png)

* 如何下载?

 * 从github下载,需要科学上网.

 https://github.com/istio/istio/releases/

 ![20220411101032](https://picgo.catface996.com/picgo20220411101032.png)

 ![20220411101308](https://picgo.catface996.com/picgo20220411101308.png)


 * 从gitee克隆.

 ![20220411101456](https://picgo.catface996.com/picgo20220411101456.png)

 gitee: https://gitee.com/catface996/istio-download

 ~~~shell
 git clone https://gitee.com/catface996/istio-download.git
 ~~~


 * 解压

 ~~~shell
 tar -xvzf istio-1.12.4-linux-amd64.tar.gz
 mv istio-1.12.4 /usr/local/istio-1.12.4
 cd /usr/local/
 ln -s istio-1.12.4/ istio
 ~~~

 * 加入环境变量
 ~~~shell
 cd /usr/local/istio
# 临时加入环境变量
export PATH=$PWD/bin:$PATH
## 永久加入环境变量,编辑/etc/profile,在最后添加export,保存后,source /etc/profile
vim /etc/profile
export PATH=/usr/local/istio/bin:$PATH
source /etc/profile
~~~

* 执行安装

* 提前准备所需镜像

~~~shell
# istio相关镜像(master节点上拉取)
istio/proxyv2                                                     1.12.4              69574f8a643d        7 weeks ago         260MB
istio/pilot                                                       1.12.4              a11fe32e6ad7        7 weeks ago         192MB

docker pull istio/proxyv2:1.12.4
docker pull istio/pilot:1.12.4
# bookinfo sample镜像(worker节点上拉取)
istio/examples-bookinfo-reviews-v3                                1.16.2              83e6a8464b84        21 months ago       694MB
istio/examples-bookinfo-reviews-v2                                1.16.2              39cff5d782e1        21 months ago       694MB
istio/examples-bookinfo-reviews-v1                                1.16.2              181be23dc1af        21 months ago       694MB
istio/examples-bookinfo-ratings-v1                                1.16.2              99ce598b98cf        21 months ago       161MB
istio/examples-bookinfo-details-v1                                1.16.2              edf6b9bea3db        21 months ago       149MB
istio/examples-bookinfo-productpage-v1                            1.16.2              7f1e097aad6d        21 months ago       207MB

docker pull istio/examples-bookinfo-reviews-v3:1.16.2
docker pull istio/examples-bookinfo-reviews-v2:1.16.2
docker pull istio/examples-bookinfo-reviews-v1:1.16.2
docker pull istio/examples-bookinfo-ratings-v1:1.16.2
docker pull istio/examples-bookinfo-details-v1:1.16.2
docker pull istio/examples-bookinfo-productpage-v1:1.16.2
# dashboard相关镜像(worker节点上拉取)
quay.io/kiali/kiali                                               v1.42               f1dca328da23        5 months ago        203MB
grafana/grafana                                                   8.1.2               b9cdc06f46e6        7 months ago        213MB
jaegertracing/all-in-one                                          1.23                2c4eb3e70f5b        10 months ago       52.9MB
prom/prometheus                                                   v2.26.0             6d6859d1a42a        12 months ago       169MB
jimmidyson/configmap-reload                                       v0.5.0              d771cc9785a1        14 months ago       9.99MB

docker pull quay.io/kiali/kiali:v1.42
docker pull grafana/grafana:8.1.2
docker pull jaegertracing/all-in-one:1.23
docker pull prom/prometheus:v2.26.0
docker pull jimmidyson/configmap-reload:v0.5.0
~~~

* 查看安装配置-profile

地址: https://istio.io/latest/docs/setup/additional-setup/config-profiles/

![20220411104034](https://picgo.catface996.com/picgo20220411104034.png)

![20220411104118](https://picgo.catface996.com/picgo20220411104118.png)

* 运行安装命令

~~~shell
istioctl install --set profile=demo -y
# 执行结果
✔ Istio core installed
✔ Istiod installed
✔ Egress gateways installed
✔ Ingress gateways installed
✔ Installation complete
~~~

* 对default命名空间开启istio注入

~~~shell
kubectl label namespace default istio-injection=enabled
namespace/default labeled
# 验证注入是否生效,通过部署一个pod来验证,我们来部署cat-dp.yaml
# 可以通过比对的方式,创建一个没有开启istio注入的命名空间,同样部署一个cat-dp

kubectl apply -f dog-dp.yaml  

kubectl describe pod dog-dpxxx

kubectl replace --force -f cat-dp.yaml
# 或者
kubectl scale --replicas=2 deployment/cat-dp

kubectl describe pod $podName
## 查看pod中的container


~~~

2. Deploy the sample application

~~~shell
[root@k8s-master-22 istio]# pwd
/usr/local/istio
# 部署bookinfo的样例
kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
# 实际执行结果
[root@k8s-master-22 istio]# kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
# 查看deployment 和 pod
[root@k8s-master-22 istio]# kubectl get deploy
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
cat-dp           2/2     2            2           39m
details-v1       1/1     1            1           3m9s
dog-dp           1/1     1            1           34m
productpage-v1   1/1     1            1           3m8s
ratings-v1       1/1     1            1           3m9s
reviews-v1       1/1     1            1           3m9s
reviews-v2       1/1     1            1           3m9s
reviews-v3       1/1     1            1           3m8s
[root@k8s-master-22 istio]#

[root@k8s-master-22 istio]# kubectl get pod
NAME                              READY   STATUS    RESTARTS   AGE
cat-dp-57585f9c77-bpdsn           2/2     Running   0          42m
cat-dp-57585f9c77-gwgkz           2/2     Running   0          27m
details-v1-79f774bdb9-4wgfw       2/2     Running   0          6m13s
dog-dp-b6c8757d4-46nlh            2/2     Running   0          37m
productpage-v1-6b746f74dc-sp4rk   2/2     Running   0          6m12s
ratings-v1-b6994bb9-5xz2f         2/2     Running   0          6m13s
reviews-v1-545db77b95-jn5cr       2/2     Running   0          6m12s
reviews-v2-7bf8c9648f-nkp4q       2/2     Running   0          6m12s
reviews-v3-84779c7bbc-glqbp       2/2     Running   0          6m12s
[root@k8s-master-22 istio]#
# 验证pod是否能够提供http服务
kubectl exec "$(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')" -c ratings -- curl -sS productpage:9080/productpage | grep -o "<title>.*</title>"
# 翻译一下: 进入 ratings 容器,执行 curl http://productpage:9080/productpage  看返回结果中是否有 <title>.*</title>
[root@k8s-master-22 istio]# kubectl exec -ti ratings-v1-b6994bb9-5xz2f curl http://productpage:9080/productpage
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
<!DOCTYPE html>
<html>
<head>
<title>Simple Bookstore App</title>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">     

<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="static/bootstrap/css/bootstrap.min.css">
...
~~~


3. Open the application to outside traffic

* 将pod和istio-gateway关联
~~~shell
# 待执行的命令
cd /usr/local/istio/
kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
# 实际的执行结果
[root@k8s-master-22 istio]# kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
gateway.networking.istio.io/bookinfo-gateway created
virtualservice.networking.istio.io/bookinfo created
~~~

*  确保配置没有问题

~~~shell
[root@k8s-master-22 istio]# istioctl analyze

✔ No validation issues found when analyzing namespace: default.
[root@k8s-master-22 istio]#
# 查看VirtualService
[root@k8s-master-22 istio]# kubectl get vs
NAME       GATEWAYS               HOSTS   AGE
bookinfo   ["bookinfo-gateway"]   ["*"]   9m48s
[root@k8s-master-22 istio]# kubectl describe vs bookinfo
Name:         bookinfo
Namespace:    default
Labels:       <none>
Annotations:  <none>
API Version:  networking.istio.io/v1beta1
Kind:         VirtualService
Metadata:
Creation Timestamp:  2022-04-11T08:37:51Z
Generation:          1
Managed Fields:
  API Version:  networking.istio.io/v1alpha3
  Fields Type:  FieldsV1
  fieldsV1:
    f:metadata:
      f:annotations:
        .:
        f:kubectl.kubernetes.io/last-applied-configuration:
    f:spec:
      .:
      f:gateways:
      f:hosts:
      f:http:
  Manager:         kubectl-client-side-apply
  Operation:       Update
  Time:            2022-04-11T08:37:51Z
Resource Version:  14625
UID:               620bc446-aed0-48c2-a7cb-9026144b5e0d
Spec:
Gateways:
  bookinfo-gateway
Hosts:
  *
Http:
  Match:
    Uri:
      Exact:  /productpage
    Uri:
      Prefix:  /static
    Uri:
      Exact:  /login
    Uri:
      Exact:  /logout
    Uri:
      Prefix:  /api/v1/products
  Route:
    Destination:
      Host:  productpage
      Port:
        Number:  9080
Events:            <none>
# 查看istio的gateway
[root@k8s-master-22 istio]# kubectl get gateway
NAME               AGE
bookinfo-gateway   10m
[root@k8s-master-22 istio]# kubectl describe gateway bookinfo-gateway
Name:         bookinfo-gateway
Namespace:    default
Labels:       <none>
Annotations:  <none>
API Version:  networking.istio.io/v1beta1
Kind:         Gateway
Metadata:
Creation Timestamp:  2022-04-11T08:37:51Z
Generation:          1
Managed Fields:
  API Version:  networking.istio.io/v1alpha3
  Fields Type:  FieldsV1
  fieldsV1:
    f:metadata:
      f:annotations:
        .:
        f:kubectl.kubernetes.io/last-applied-configuration:
    f:spec:
      .:
      f:selector:
        .:
        f:istio:
      f:servers:
  Manager:         kubectl-client-side-apply
  Operation:       Update
  Time:            2022-04-11T08:37:51Z
Resource Version:  14624
UID:               df23873c-18d9-4e9b-94da-2724471e74ac
Spec:
Selector:
  Istio:  ingressgateway
Servers:
  Hosts:
    *
  Port:
    Name:      http
    Number:    80
    Protocol:  HTTP
Events:          <none>
~~~

* 访问bookinfo服务

* 通过Ingress访问

没有安装,需要在k8s集群中安装Ingress方能支持.

* 通过NodePort访问
~~~shell
[root@k8s-master-22 istio]# kubectl get svc istio-ingressgateway -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                      AGE
istio-ingressgateway   LoadBalancer   10.10.210.169   <pending>     15021:30523/TCP,80:32177/TCP,443:32262/TCP,31400:30612/TCP,15443:31112/TCP   128m
[root@k8s-master-22 istio]#
# 可以看到 80:32177/TCP 中的 32177 即为NodePort,可以通过describe命令进一步确认
[root@k8s-master-22 istio]# kubectl describe service  -n istio-system istio-ingressgateway
Name:                     istio-ingressgateway
Namespace:                istio-system
Labels:                   app=istio-ingressgateway
                          install.operator.istio.io/owning-resource=unknown
                          install.operator.istio.io/owning-resource-namespace=istio-system
                          istio=ingressgateway
                          istio.io/rev=default
                          operator.istio.io/component=IngressGateways
                          operator.istio.io/managed=Reconcile
                          operator.istio.io/version=1.12.4
                          release=istio
Annotations:              <none>
Selector:                 app=istio-ingressgateway,istio=ingressgateway
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.10.210.169
IPs:                      10.10.210.169
Port:                     status-port  15021/TCP
TargetPort:               15021/TCP
NodePort:                 status-port  30523/TCP
Endpoints:                10.122.158.3:15021
Port:                     http2  80/TCP
TargetPort:               8080/TCP
NodePort:                 http2  32177/TCP
Endpoints:                10.122.158.3:8080
Port:                     https  443/TCP
TargetPort:               8443/TCP
NodePort:                 https  32262/TCP
Endpoints:                10.122.158.3:8443
Port:                     tcp  31400/TCP
TargetPort:               31400/TCP
NodePort:                 tcp  30612/TCP
Endpoints:                10.122.158.3:31400
Port:                     tls  15443/TCP
TargetPort:               15443/TCP
NodePort:                 tls  31112/TCP
Endpoints:                10.122.158.3:15443
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
[root@k8s-master-22 istio]#
~~~

浏览器访问

通过master节点的nodePort访问
![20220411173430](https://picgo.catface996.com/picgo20220411173430.png)

通过worker节点的nodePort访问
![20220411173529](https://picgo.catface996.com/picgo20220411173529.png)


4. View the dashboard

* 安装Kiali 和其他插件并等待它们被部署

~~~shell
# 待执行的部署命令
kubectl apply -f samples/addons
# 实际执行结果
[root@k8s-master-22 istio]# kubectl apply -f samples/addons
serviceaccount/grafana created
configmap/grafana created
service/grafana created
deployment.apps/grafana created
configmap/istio-grafana-dashboards created
configmap/istio-services-grafana-dashboards created
deployment.apps/jaeger created
service/tracing created
service/zipkin created
service/jaeger-collector created
serviceaccount/kiali created
configmap/kiali created
clusterrole.rbac.authorization.k8s.io/kiali-viewer created
clusterrole.rbac.authorization.k8s.io/kiali created
clusterrolebinding.rbac.authorization.k8s.io/kiali created
role.rbac.authorization.k8s.io/kiali-controlplane created
rolebinding.rbac.authorization.k8s.io/kiali-controlplane created
service/kiali created
deployment.apps/kiali created
serviceaccount/prometheus created
configmap/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
service/prometheus created
deployment.apps/prometheus created
[root@k8s-master-22 istio]#
# 查看部署后的pod
[root@k8s-master-22 istio]# kubectl get pod -n istio-system
NAME                                    READY   STATUS    RESTARTS   AGE
grafana-6ccd56f4b6-6tj65                1/1     Running   0          32s
istio-egressgateway-56f4569d45-5jnq5    1/1     Running   0          3h7m
istio-ingressgateway-786c6bddb7-ntldl   1/1     Running   0          3h7m
istiod-84f87dcc49-tmqbf                 1/1     Running   0          3h7m
jaeger-5d44bc5c5d-s9whj                 1/1     Running   0          32s
kiali-79b86ff5bc-c72k6                  1/1     Running   0          32s
prometheus-64fd8ccd65-qhv7g             2/2     Running   0          32s
~~~


* 访问 Kiali 仪表板

注意:需要绑定服务暴露的ip地址 --address 192.168.162.22
~~~shell
# 查看 istioctl dashboard 帮助文档
[root@k8s-master-22 istio]# istioctl dashboard --help

Flags:
--address string   Address to listen on. Only accepts IP address or localhost as a value. When localhost is supplied, istioctl will try to bind on both 127.0.0.1 and ::1 and will fail if neither of these address are available to bind. (default "localhost")
# 需要执行的命令 
istioctl dashboard kiali --address 192.168.162.22

[root@k8s-master-22 istio]# istioctl dashboard kiali --address 192.168.162.22
http://192.168.162.22:20001/kiali
Failed to open browser; open http://192.168.162.22:20001/kiali in your browser.
~~~

![20220411180254](https://picgo.catface996.com/picgo20220411180254.png)

通过浏览器多访问几次 http://192.168.162.221:32177/productpage 后

![20220411180423](https://picgo.catface996.com/picgo20220411180423.png)
## 挖坑记录

containerd是什么鬼?

github:  https://github.com/containerd/containerd

官网: https://containerd.io/



CRI-O 这又是个什么鬼?

github:  https://github.com/cri-o/cri-o


netfilter 和 iptables

netfilter官网: https://www.netfilter.org/

什么是netfilter?
![20220408103936](https://picgo.catface996.com/picgo20220408103936.png)

什么是iptables?
![20220408104106](https://picgo.catface996.com/picgo20220408104106.png)

参考文档:

https://www.zsythink.net/archives/1199


继续挖坑,等待后续视频填坑...